# -*- coding: utf-8 -*-
"""
:author: Tim HÃ¤berlein
:version: 1.0
:date: 21.10.2025
:organisation: TU Dresden, FZM
"""

# -------- start import block ---------
import argparse
import sys
import pandas as pd
from .webcrawler import AVAILABLE_CRAWLERS

# -------- /import block ---------
"""
read_transactions CLI
---------------------
Startet Crawler direkt Ã¼ber die Kommandozeile.

Beispiele:
    python -m read_transactions list
    python -m read_transactions run ariva --start 01.01.2024 --end 31.01.2024
"""


# AVAILABLE_CRAWLERS = {
#     "ariva": ArivaCrawler,
#     "amex": AmexCrawler, ...}

def list_crawlers() -> None:
    """Zeigt alle verfÃ¼gbaren Crawler aus der Registry an."""
    if not AVAILABLE_CRAWLERS:
        print("âš ï¸  Keine Crawler registriert.")
        return

    print("VerfÃ¼gbare Crawler:")
    for key in AVAILABLE_CRAWLERS:
        print(f"  - {key}")
    print()


def run_crawler(name: str, start: str, end: str, log_level:str) -> None:
    """Startet den angegebenen Crawler."""
    if name not in AVAILABLE_CRAWLERS:
        print(f"âŒ Unbekannter Crawler: {name}")
        print("VerfÃ¼gbare Optionen:")
        for key in AVAILABLE_CRAWLERS:
            print(f"  - {key}")
        sys.exit(1)

    crawler_cls = AVAILABLE_CRAWLERS[name]

    print(f"ğŸš€ Starte {name}-Crawler ...")
    with crawler_cls(start_date=start, end_date=end, logging_level=log_level) as crawler:
        try:
            crawler.login()
            crawler.download_data()
            crawler.process_data()
            crawler.save_data()
        except Exception as e:
            print(f"âŒ Fehler wÃ¤hrend der AusfÃ¼hrung: {e}")
            sys.exit(1)

    print(f"âœ… {name}-Crawler abgeschlossen.\n")


# -------------------------------------------------------------------
# Hauptfunktion
# -------------------------------------------------------------------

def main() -> None:
    parser = argparse.ArgumentParser(
        prog="read_transactions",
        description="CLI fÃ¼r das read_transactions-Projekt â€“ verwaltet und startet Crawler.",
    )
    subparsers = parser.add_subparsers(dest="command", help="VerfÃ¼gbare Befehle")

    # --- list command ---
    subparsers.add_parser("list", help="Listet alle verfÃ¼gbaren Crawler auf")

    # --- run command ---
    parser_run = subparsers.add_parser("run", help="Startet einen bestimmten Crawler")
    parser_run.add_argument("name", help="Name des Crawlers (z. B. ariva)")
    parser_run.add_argument("--start", type=str, help="Startdatum (dd.mm.yyyy)",
                            default=pd.to_datetime("today").strftime('%d.%m.%Y')
                            )
    parser_run.add_argument("--end", type=str, help="Enddatum (dd.mm.yyyy)",
                            default=(pd.to_datetime("today")-pd.DateOffset(months=6)).strftime('%d.%m.%Y')
                            )
    parser_run.add_argument("--log_level", type=str, help="Log_Level (DEBUG, INFO, WARNING, ERROR)",
                            default="INFO")
    args = parser.parse_args()

    if args.command == "list":
        list_crawlers()
    elif args.command == "run":
        run_crawler(args.name, args.start, args.end, args.log_level)
    else:
        parser.print_help()


if __name__ == "__main__":
    main()

